◇課題L.１～L.8
特に問題なし

◇課題L.9
(i)何を行ったか
ファイルから読み込んだ一文字が空白だった時にtokenに入れず読み飛ばす
更にそのアクションを空白が終わるまで読み飛ばす
こうすることでlex_skip_space()関数の処理理が終わった時には自然と
次の読むのは何かしらの文字になるし実際

(ii)結果
四行目まで空白のみなのでひたすら読み飛ばし、５行目から文字なので
その時はまだほかの字句解析ができない状況だったので止まったことが
わかった

(iii)感想
(i)のように処理を行うことでlex_skip_space()関数の処理理が終わった時
には自然と次に読むのは何かしらの文字になるし実際lex.cのコードを見てみ
ると、lex_skip_spaceの処理の下にはkeywordやdigitを読み込みtokenに
入れる分岐処理が行われているのでそういうことだとわかった

◇課題L.10
(i)何を行ったか
ファイルから読み込んだ文字一文字目がアルファベットだった場合、二文字
目以降、アルファベットか数字である限り、tokenに入れていき、入れ終わっ
たらその出来上がったtokenが予想される識別子かキーワードのどれかを
strcmp関数を用いて逐次文字配列同士の比較で探す

(ii)結果
課題L.9では５行目に入った瞬間止まったが、今度は５行目まで処理が進み
６行目でとまった

(iii)感想
最初は何をしているのかよくわからなかったが、結局トークンで考えられる
識別子かキーワードか、数字か、文字リテラルかその他でtokenへの文字の
入れ方が変わるから最初の一文字目を参考に、どのトーケンタイプか当たり
をつけて、それに応じて場合分けしているということが分かった。
L.10の関数はそのうちの識別子、キーワード担当で、最初の一文字アルファベット
二文字目以降数字orアルファベットの塊をトークンとして切り出し、その
切り出したtokenが具体的に識別子のうちのどれかをif文で分岐処理し、
どれにも当てはまらなかったら、残りのキーワードであろうということで
else処理を行っていることが分かった

◇課題L.11
(i)何を行ったか
'_'もアルファベットとして判定してくれるマクロ関数isalpha_(char)を
defineを用いて定義した

(ii)結果
lex_get_kw_id関数が"_program"や"u_p0x_AOx_774"のような'_'を伴う
キーワードもしっかりキーワードとして認識するようになった

(iii)感想
課題自体は簡単だったが、実際に一からプログラムを自分で考える場合には
キーワードには'_'を含むキーワードがある
↓
でもisalpha()という標準識別子では'_'を文字認識しない
↓
isalpha()を文字かどうかを判断するために使っているlex_get_kw_id関数
ではその手のキーワードが認識できない
↓
新しいマクロ関数を定義しよう
という風に細かいことに気づき、フィードバックする必要があるので、難しい
ことだとわかった

◇課題L.12
(i)何を行ったか
tokenを切り出し終わり新しく読みだした文字の先頭が数字だった時の別の
tokenの切り出し方をするk関数lex_get_intを定義し具体的な処理を記入
tokenのタイプはINTと確定して、次の文字(=数字)を読みだしては、すでに
読みだしている値の桁を一つ左にずらしながら加えていく処理を数字を読み
終えるまで続けtokenとして切り出しをする

(ii)結果
６行目で止まっていたが７行目まで進み、値を読み出しながら同じ量の値が
valに格納されていることが分かった

(iii)感想
lex_get_kw_idのようにtokenのタイプを細かく吟味せず、関数内で頭が
数字と分かった時点でタイプをINTと決めつけているのは数字はINTしか
考えられず、頭がアルファベットから始まる数字はないからだと分かった。
また、読み込み終えた値の桁を左にずらしながら新しく加えていく発想は
分かりやすかったが、新しく読んだ数字をそのまま代入せず、"x->c-'0'"
とするところが分からなかった
推測では、構造体lex_tの要素であるcはchar型で宣言されているため、
c_getでファイルから読み出しcに代入、つまり読み出すときに、読み出し
ているのは数字なのにかかわらず、文字として読みだしているため、
valに値として入力したいときにはx->c(文字)を値にいわゆる型変換、
キャストをするために"-'0'"のように数字の０を引いたと考える

◇課題L.13
(i)何を行ったか
L.10, L.1212同様文字リテラルをtokenに切り出す担当の関数lex_get_char
を新たに定義し、具体的な処理を描いた
L.10のように文字リテラルの中にもtokenタイプは様々考えられるため
細かく分岐処理を行って吟味をしている

(ii)結果
更に８行目まで解析が進んだ

(iii)感想
やっていることはL.10に近い印象を受けた。
また文字リテラルということでtokenとしては今までのように2,3,4...
文字と複数文字にならず、一文字なのでwhile()などで文字の読出しを
繰り返す必要がないことが分かった

◇課題L.14
(i)何を行ったか
最後に上の関数達で処理しきれない、”以外”のtokenのタイプを処理
するlex_get_othersの関数を定義、処理をかいた

(ii)結果
新たに９行目まで解析が進んだ

(iii)感想
これもL.13同様、一文字なのでその一文字について、その一文字と対応
するタイプを出力するように逐次ifで分岐処理をした

◇課題L.15
(i)何を行ったか
L.14では一文字で一トークンになるようなtokenを相手にしたが、L.15では
二文字で一トークンになるようなtokenを切り出す操作をlex_get_others
関数後半で定義した。

(ii)結果
testlex.txt内のすべての解析ができるようになった

(iii)感想
二文字になったため、全体的な印象としてはL.14の{if, else if ... else}
ではなく{if{if, else}, if{if, else}...else}という風に二段構成の条件
分岐でtokenのタイプを考えていることが分かった。
